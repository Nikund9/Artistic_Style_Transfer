{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5DLXKJrV2Knuv8701IvOb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikund9/Artistic_Style_Transfer/blob/main/Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural style transfer\n",
        "Neural style transfer employs an optimization method that merges two distinct images: a content image and a style reference image (e.g., an artwork created by a renowned painter). Through this technique, these images are combined to generate an output that resembles the content image while adopting the artistic style depicted in the style reference image."
      ],
      "metadata": {
        "id": "7_CZ0ekfJ1eZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LOUN38BmAeVC"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG19 Model Initialization\n",
        "Within this segment, we begin by setting up a VGG19 model integrated with pre-existing weights utilizing the torchvision library. Renowned for its prowess in image classification, VGG19 stands as a convolutional neural network. In this context, we focus on extracting its feature layers, which will be instrumental in facilitating subsequent steps in Neural Style Transfer."
      ],
      "metadata": {
        "id": "qsdOhQxLKMoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg19(pretrained = True).features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUZSb3ZpArrC",
        "outputId": "50db9a85-ae24-4584-90f9-daf4d0950441"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:05<00:00, 99.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2PnHhPoAyJN",
        "outputId": "d57f9a69-6818-4e60-8640-307c3329a0f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): ReLU(inplace=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (31): ReLU(inplace=True)\n",
            "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (33): ReLU(inplace=True)\n",
            "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (35): ReLU(inplace=True)\n",
            "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom VGG Model and Image Loading\n",
        "In this section, we craft a customized VGG model optimized for Neural Style Transfer (NST). This specialized model is designed to extract crucial features from particular layers (['0', '5', '10', '19', '28']) of a pre-trained VGG19 model. Additionally, a utility function named 'load_image' is created to streamline the process of loading and pre-processing images.\n",
        "\n",
        "Our approach involves the removal of all fully connected dense layers present at the conclusion of VGG19, retaining solely the convolutional layers (Blocks 1 through 5). As per the paper's specifications, each convolutional block preceding a maxpool layer is grouped together, resulting in five distinct groups. We select the output from the convolutional layer just before each maxpool layer, denoted as layers '0', '5', '10', '19', and '28' respectively, aligning with the specified groupings in the paper."
      ],
      "metadata": {
        "id": "jdfBGKbUKktE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.chosen_features = ['0','5','10','19','28']\n",
        "        self.model = models.vgg19(pretrained = True).features[:29]\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for layer_number, layer in enumerate(self.model):\n",
        "            x = layer(x)\n",
        "            if str(layer_number) in self.chosen_features:\n",
        "                features.append(x)\n",
        "        return features\n",
        "def load_image(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "image_size = 356\n",
        "loader = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "lcGGpTswA2SI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Images\n",
        "Load the base image and reference image where reference image is the image from where we will be taking the artistic style."
      ],
      "metadata": {
        "id": "x_lXGCUbK4KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_image = load_image(\"base_image.jpg\").to(device)\n",
        "artistic_image = load_image(\"reference_image.jpg\").to(device)\n",
        "model.to(device)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_LccjfuA6L1",
        "outputId": "5e351260-0021-4e9d-a7f0-e882f40fba5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (17): ReLU(inplace=True)\n",
              "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (24): ReLU(inplace=True)\n",
              "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (26): ReLU(inplace=True)\n",
              "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (31): ReLU(inplace=True)\n",
              "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (33): ReLU(inplace=True)\n",
              "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (35): ReLU(inplace=True)\n",
              "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the generated image as a clone of the original image\n",
        "generated = original_image.clone().requires_grad_(True)\n",
        "# Load the VGG model for feature extraction and set it to evaluation mode\n",
        "model = VGG().to(device).eval()"
      ],
      "metadata": {
        "id": "YN54NlScBDp6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters for the optimization process\n",
        "total_steps = 4000\n",
        "Alpha = 1\n",
        "Beta = 0.06\n",
        "optimizer = optim.Adam([generated], lr = 0.001)\n",
        ""
      ],
      "metadata": {
        "id": "V7xejmLpCA9c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization Loop\n",
        "This code block defines the optimization loop for Neural Style Transfer. The generated image is iteratively updated to minimize the total loss, which is a combination of content and style losses."
      ],
      "metadata": {
        "id": "mqii-pHrLHQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(total_steps):\n",
        "# Obtain features from chosen layers for the generated, original, and style images\n",
        "  generated_features = model(generated)\n",
        "  original_image_features = model(original_image)\n",
        "  artistic_image_features = model(artistic_image)\n",
        "# Initialize style loss and original loss\n",
        "  artistic_loss = 0\n",
        "  original_loss = 0\n",
        "# Iterate through the features for chosen layers\n",
        "  for gf, of, af in zip(generated_features,\n",
        "                        original_image_features, artistic_image_features):\n",
        "# Calculate the mean squared error for original image and generated image features\n",
        "    batchSize, channel, height, width = gf.shape\n",
        "    original_loss += torch.mean((gf - of) ** 2)\n",
        "\n",
        "# Compute Gram Matrix for generated and style images and calculate style loss\n",
        "    g_l = gf.view(channel, height * width).mm(\n",
        "        gf.view(channel, height*width).t()\n",
        "    )\n",
        "    a_l = af.view(channel, height*width).mm(\n",
        "        af.view(channel, height*width).t()\n",
        "    )\n",
        "    artistic_loss += torch.mean((g_l - a_l)**2)\n",
        "  # Calculate total loss using the defined weights (alpha and beta)\n",
        "  total_loss = Alpha * original_loss + Beta * artistic_loss\n",
        "  # Clear gradients, perform backpropagation, and update weights\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "# Print total loss and save the generated image at certain intervals\n",
        "  if step % 200 == 0:\n",
        "    print(total_loss)\n",
        "    save_image(generated, \"generated.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoMUQ50ICCvz",
        "outputId": "afbdc512-4c09-4353-8650-9b7b14cfafc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(14383.4844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(13681.3633, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(12670.9434, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11627.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(10622.8682, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9700.2217, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8900.6133, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8230.5977, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(7648.2114, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(7157.3389, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6733.5596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6400.2446, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(6060.2412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5782.8994, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5559.3330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5368.3989, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(5134.9370, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4952.0552, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4801.7090, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(4644.6050, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wN1RsgkACGJH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}